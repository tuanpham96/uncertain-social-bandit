{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db633a1e-0109-40ae-aa33-e209ea0eddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"notebooks\" in os.getcwd(): os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d67331-aedf-4a48-9d13-d3fc2bc5b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import numpy.random as npr\n",
    "import networkx as nx \n",
    "\n",
    "import warnings\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "from itertools import product as iterprod\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import yaml\n",
    "\n",
    "import socialbandit as sb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb08d8e-014d-4d3d-ad2e-4279c991809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams, font_manager\n",
    "\n",
    "rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use('default')\n",
    "\n",
    "# use one of the available styles \n",
    "plt.style.use('seaborn-notebook')\n",
    "\n",
    "# further customization\n",
    "rcParams['font.family'] = 'FreeSans'\n",
    "rcParams['font.size'] = 12\n",
    "\n",
    "rcParams['axes.titlesize'] = 14\n",
    "rcParams['axes.labelsize'] = 12\n",
    "rcParams['legend.fontsize'] = 12\n",
    "rcParams['xtick.labelsize'] = 12\n",
    "rcParams['ytick.labelsize'] = 12\n",
    "rcParams['axes.linewidth'] = 1.5\n",
    "\n",
    "rcParams['mathtext.fontset'] = 'cm' \n",
    "rcParams['mathtext.rm'] = 'serif'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ff2dfb-9da6-419c-8a3f-624aa10f9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prevchoice_diff(A_prev, A_curr):\n",
    "    # A_*: binary matrices of size \"num_tasks x num_agents\"\n",
    "    # A_prev could also be cumulative previous A\n",
    "    return np.sum(A_prev * A_curr, axis=0) < 1.0\n",
    "    \n",
    "def get_task_choices(A):\n",
    "    inds = np.where(A > 0)\n",
    "    return inds[0][np.argsort(inds[1])]\n",
    "\n",
    "def get_task_rewards(Y):\n",
    "    return Y.sum(axis=0)\n",
    "\n",
    "def aggregate_rewards(rewards, condition):\n",
    "    num = np.mean(condition.sum(axis=0))\n",
    "    mag = np.mean(np.abs(rewards[condition])) if num > 0 else 0.0\n",
    "    return dict(num=num, mag=mag)\n",
    "\n",
    "def calculate_entropy(X):\n",
    "    _, n = np.unique(X, return_counts=True)\n",
    "    P = n/sum(n)\n",
    "    return np.sum(-P * np.log2(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2baacb4-91e7-4c8d-875d-b0471ed2ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoAverageCumTime:\n",
    "    # this is a pseduo class to create a function for time average with a persistent time variable\n",
    "    # needed for when using a function with only one argument and no internal place for time update\n",
    "    _t = -1\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._t = 0\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        self._t += 1\n",
    "        return X / self._t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544a3a8d-b6d1-4e03-9599-5b918c9692eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialMAB_ChildDev(sb.models.SocialMultiArmedBandit):\n",
    "    def __init__(self, \n",
    "                 num_agents=20, \n",
    "                 max_time=1200,\n",
    "                 social_content = 'belief', \n",
    "                 social_network = None,\n",
    "                 gamma = 1,\n",
    "                 mu0 = 100.0, \n",
    "                 var0 = 40.0, \n",
    "                 softmax_tau = 1.0, \n",
    "                 BMT_error = 'use-tasks', \n",
    "                 agg_explore_step=50, \n",
    "                 agg_reward_step=400):\n",
    "        \n",
    "        if type(gamma) is list:\n",
    "            if len(gamma) != 2: \n",
    "                raise ValueError(\"`gamma` can only either be a 2-element list or a scalar\")\n",
    "        else:\n",
    "            if gamma < 0 or gamma > 1:\n",
    "                raise ValueError(\"`gamma` needs to be in [0, 1] if it's a scalar input\")\n",
    "            gamma = [gamma, 1.0 - gamma]\n",
    "            \n",
    "        self.gamma_acl = gamma[0] # for action learning scaling\n",
    "        self.gamma_sol = gamma[1] # for social learning scaling\n",
    "        \n",
    "        # Social content for social influence\n",
    "        social_content_opts = {\n",
    "            'belief': 'M',\n",
    "            'action': 'A',\n",
    "            'reward': 'Y', \n",
    "            'cum_reward': 'Yc',\n",
    "            'mean_reward': ('Yc', PseudoAverageCumTime())\n",
    "        }\n",
    "        \n",
    "        social_content = social_content.lower()\n",
    "        if social_content not in social_content_opts: \n",
    "            raise ValueError(\"`social_content` cannot be {social_content}. Available options are {list(social_content_opts.keys())}.\")\n",
    "\n",
    "        content_fn = sb.social.ContentConstructor(*social_content_opts[social_content])\n",
    "        \n",
    "        # Social network constructor/functions\n",
    "        social_fn = None\n",
    "        if social_network is not None:                \n",
    "            social_fn = sb.social.StaticSocialNetwork(W = social_network)\n",
    "        \n",
    "        # Social setting constructur from social content and network        \n",
    "        social_settings = sb.social.SocialSetting(\n",
    "            N = num_agents, \n",
    "            social_fn = social_fn,\n",
    "            content_fn = content_fn\n",
    "        )\n",
    "        \n",
    "        # Social learner constructor\n",
    "        social_learner = None\n",
    "        if social_network is not None:\n",
    "            social_learner = sb.learners.MeanFriendContentLearner()\n",
    "        \n",
    "        # Childhood development settings\n",
    "        task_settings = sb.tasks.ChildDevelopmentEnvironment(env_levels = 12)\n",
    "        \n",
    "        # Initializer\n",
    "        initializer = sb.initializers.InitializerBanditAgnostic(\n",
    "            mu_fn = lambda x: mu0,\n",
    "            sigma2_fn = lambda x: var0\n",
    "        )\n",
    "        \n",
    "        # Sampler\n",
    "        action_sampler = sb.tasks.SoftmaxSampler(tau = softmax_tau)\n",
    "        \n",
    "        # Bayesian mean tracker to update belief\n",
    "        if type(BMT_error) is str:\n",
    "            if BMT_error.lower() == 'use-tasks':\n",
    "                bmt_var_err = task_settings\n",
    "            else: \n",
    "                try:\n",
    "                    bmt_var_err = float(BMT_error)\n",
    "                except:\n",
    "                    raise ValueError(\"If `BMT_error` is a string, only 'use-tasks' or a string of a float is accepted at this point\")\n",
    "        elif type(BMT_error) in [float, int]:\n",
    "            bmt_var_err = BMT_error\n",
    "        else:\n",
    "            raise ValueError(\"`BMT_error` can either be a str/float/int\")\n",
    "        \n",
    "        belief_updater  = sb.beliefs.BayesianMeanTracker(var_error = bmt_var_err)\n",
    "        \n",
    "        # Initialize \n",
    "        super().__init__(\n",
    "            task_settings   = task_settings,\n",
    "            social_settings = social_settings,\n",
    "            clock           = sb.utils.ExperimentClock(T = max_time),\n",
    "            initializer     = initializer,\n",
    "            action_sampler  = action_sampler,\n",
    "            belief_updater  = belief_updater,\n",
    "            action_learner  = sb.learners.MeanGreedyExploit(),\n",
    "            social_learner  = social_learner\n",
    "        )\n",
    "        \n",
    "        self.set_state_managers()\n",
    "        \n",
    "        # Set up analyses variables\n",
    "        analysis_size = (max_time+1, num_agents)\n",
    "        cum_choice_size = (self.num_tasks, num_agents)\n",
    "        self.analysis = dict(\n",
    "            params = dict(\n",
    "                agg_explore_step = agg_explore_step, \n",
    "                agg_reward_step = agg_reward_step\n",
    "            ),\n",
    "            per_agent = dict(\n",
    "                cum_choice = np.zeros(cum_choice_size),\n",
    "                explore = np.zeros(analysis_size),\n",
    "                choice = np.zeros(analysis_size),\n",
    "                reward = np.zeros(analysis_size)\n",
    "            ),\n",
    "            time = dict(\n",
    "                explore = range(0, max_time, agg_explore_step),\n",
    "                reward = np.array(range(0, max_time, agg_reward_step)) + agg_reward_step,\n",
    "            ),\n",
    "            aggregate = dict(\n",
    "                explore_num = [],\n",
    "                unq_choices = [],\n",
    "                explore_ent = [], \n",
    "                mean_reward = [],\n",
    "                loss_num = [],\n",
    "                loss_mag = [],\n",
    "                gain_num = [],\n",
    "                gain_mag = []                \n",
    "            ),\n",
    "            aux = dict(\n",
    "                Q_acl = np.zeros(max_time+1), \n",
    "                Q_sol = np.zeros(max_time+1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def update_utility(self):\n",
    "        Q_acl = self.gamma_acl * self.learn_action()\n",
    "        Q_sol = self.gamma_sol * self.learn_social()\n",
    "        self.states.Q = Q_acl + Q_sol\n",
    "\n",
    "    def analyze(self):\n",
    "        t = self.t\n",
    "        \n",
    "        # auxilary \n",
    "        aux_var = self.analysis['aux']\n",
    "        aux_var['Q_acl'][t] = np.mean(self.states.Q_acl)\n",
    "        aux_var['Q_sol'][t] = np.mean(self.states.Q_sol)\n",
    "        \n",
    "        # analysis\n",
    "        anly_per_agent = self.analysis['per_agent']\n",
    "        anly_params = self.analysis['params']\n",
    "        agg_anly = self.analysis['aggregate']\n",
    "        \n",
    "        anly_per_agent['explore'][t,:] = is_prevchoice_diff(anly_per_agent['cum_choice'], self.states.A)\n",
    "        anly_per_agent['cum_choice'] += self.states.A\n",
    "        \n",
    "        anly_per_agent['choice'][t,:] = get_task_choices(self.states.A)\n",
    "        anly_per_agent['reward'][t,:] = get_task_rewards(self.states.Y)\n",
    "        \n",
    "        agg_explore_step = anly_params['agg_explore_step']\n",
    "        if t % agg_explore_step == 0 and t > 1:\n",
    "            agg_anly['explore_num'].append(\n",
    "                np.mean(anly_per_agent['explore'][t-agg_explore_step:t,:].sum(axis=0))        \n",
    "            )\n",
    "            \n",
    "            # choice_matrix = anly_per_agent['choice'][t-agg_explore_step:t,:]\n",
    "            choice_matrix = anly_per_agent['choice'][:t,:]\n",
    "            agg_anly['explore_ent'].append(\n",
    "                np.mean([calculate_entropy(X) for X in choice_matrix.T])                \n",
    "            )\n",
    "            \n",
    "            agg_anly['unq_choices'].append(\n",
    "                np.mean([len(np.unique(X)) for X in choice_matrix.T])     \n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "        agg_reward_step = anly_params['agg_reward_step']\n",
    "        if t % agg_reward_step == 0 and t > 0:\n",
    "            reward_matrix = anly_per_agent['reward'][t-agg_reward_step:t]\n",
    "            agg_anly['mean_reward'].append(np.mean(reward_matrix))\n",
    "            agg_loss = aggregate_rewards(reward_matrix, reward_matrix < 0)\n",
    "            \n",
    "            agg_anly['loss_num'].append(agg_loss['num'])\n",
    "            agg_anly['loss_mag'].append(agg_loss['mag'])\n",
    "            \n",
    "            agg_gain = aggregate_rewards(reward_matrix, reward_matrix > 0)\n",
    "            \n",
    "            agg_anly['gain_num'].append(agg_gain['num'])\n",
    "            agg_anly['gain_mag'].append(agg_gain['mag'])\n",
    "            \n",
    "        if t == self.clock.T:\n",
    "            for k, v in agg_anly.items():\n",
    "                agg_anly[k] = np.array(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7eac32d-658c-4474-a21d-4bb15441a996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ec8469cd28436394459c7261cb165b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'explore_num': array([3.024e+01, 5.620e+00, 1.400e-01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 4.036e+01, 2.866e+01, 1.682e+01, 9.020e+00,\n",
       "        5.300e+00, 2.780e+00, 1.920e+00, 1.240e+00, 5.600e-01, 3.400e-01,\n",
       "        3.400e-01, 2.400e-01, 4.000e-02, 8.000e-02, 6.000e-02, 6.000e-02]),\n",
       " 'unq_choices': array([ 31.24,  36.86,  37.  ,  37.  ,  37.  ,  37.  ,  37.  ,  37.  ,\n",
       "         76.98, 105.46, 122.1 , 131.02, 136.28, 139.  , 140.88, 142.1 ,\n",
       "        142.66, 143.  , 143.34, 143.58, 143.62, 143.7 , 143.76, 143.82]),\n",
       " 'explore_ent': array([4.81734981, 5.06035225, 5.09814353, 5.10198334, 5.09928117,\n",
       "        5.09800184, 5.09393596, 5.08952144, 5.58755828, 5.96447655,\n",
       "        6.22444775, 6.39619824, 6.50953981, 6.58217633, 6.63066547,\n",
       "        6.65310997, 6.65773001, 6.64645638, 6.62557579, 6.59856009,\n",
       "        6.5699984 , 6.53955454, 6.50698561, 6.47351491]),\n",
       " 'mean_reward': array([10.23428033, 43.81346782, 76.95531133]),\n",
       " 'loss_num': array([152.22, 110.68,  50.94]),\n",
       " 'loss_mag': array([29.70011944, 68.2995958 , 58.41856057]),\n",
       " 'gain_num': array([246.78, 289.32, 349.06]),\n",
       " 'gain_mag': array([34.90827584, 86.70256599, 96.71106975])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_agents = 50\n",
    "\n",
    "agent_social_network = nx.to_numpy_array(nx.erdos_renyi_graph(num_agents, p = 0.9))\n",
    "# agent_social_network = nx.to_numpy_array(nx.barabasi_albert_graph(num_agents, m=5))\n",
    "# agent_social_network = None \n",
    "\n",
    "model = SocialMAB_ChildDev(\n",
    "    num_agents = num_agents,\n",
    "    social_content = 'belief', \n",
    "    social_network = agent_social_network,\n",
    "    gamma = 0.5,\n",
    "    BMT_error = '3600',\n",
    ")\n",
    "\n",
    "model.run(tqdm_fn = tqdm)\n",
    "model.analysis['aggregate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50b50b3c-7a87-491f-8020-a3e47f8d2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variations(d, add_id=True, num_trials=1):\n",
    "    keys = list(d.keys())\n",
    "    values = list(d.values())\n",
    "    \n",
    "    if not add_id:\n",
    "        df = pd.DataFrame(itertools.product(*values), columns=keys)\n",
    "        return df.to_dict('records') * num_trials\n",
    "    \n",
    "    trial_ids = list(range(num_trials))\n",
    "    \n",
    "    combos = list(itertools.product(*values)) # first, combinations of values\n",
    "    combos = [list(x) + [i] for i, x in enumerate(combos)] # then, add a variation ID\n",
    "    combos = list(itertools.product(combos, trial_ids)) # duplicating with trials\n",
    "    combos = [list(x[0]) + [x[1], i] for i, x in enumerate(combos)] # lastly, add unique ID\n",
    "    \n",
    "    df = pd.DataFrame(combos, columns=keys + ['_var_id', '_trial_id', '_unq_id'])\n",
    "\n",
    "    return df.to_dict('records')\n",
    "\n",
    "def experiment_ER(num_agents, social_content, p_social, utility_gamma, BMT_error, mu0=100.0, *args, **kwargs):\n",
    "    \n",
    "    social_network = None\n",
    "    if p_social > 0:\n",
    "        social_network = nx.erdos_renyi_graph(num_agents, p = p_social)\n",
    "        social_network = nx.to_numpy_array(social_network)\n",
    "    \n",
    "    model = SocialMAB_ChildDev(\n",
    "        num_agents = num_agents,\n",
    "        social_content = social_content, \n",
    "        social_network = social_network,\n",
    "        gamma = utility_gamma,\n",
    "        BMT_error = BMT_error,\n",
    "        mu0 = mu0\n",
    "    )\n",
    "\n",
    "    model.run(tqdm_fn=None)\n",
    "    \n",
    "    result_keys = dict(\n",
    "        explore = ['explore_num', 'unq_choices', 'explore_ent'],\n",
    "        reward = ['mean_reward', 'loss_num', 'loss_mag', 'gain_num', 'gain_mag']\n",
    "    )\n",
    "    \n",
    "    agg_anly = model.analysis['aggregate']\n",
    "    agg_time = model.analysis['time']\n",
    "    \n",
    "    results = {}\n",
    "    for section_key, agg_keys in result_keys.items():\n",
    "        results[section_key] = {k: v for k, v in agg_anly.items() \n",
    "                                if k in agg_keys}\n",
    "        results[section_key].update({'time': agg_time[section_key]})\n",
    "        results[section_key] = pd.DataFrame(results[section_key])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(file_prefix, results, variations, var_dict):\n",
    "    results = {k: pd.concat([x[k] for x in results], ignore_index=True)\n",
    "               for k in results[0].keys()}\n",
    "    results.update({'variations': pd.DataFrame(variations)})\n",
    "    \n",
    "    for k, df in results.items(): \n",
    "        file_name = f'{file_prefix}-{k}.parq'\n",
    "        df.to_parquet(file_name, engine='fastparquet')\n",
    "    \n",
    "    with open(f'{file_prefix}-variations.yaml', 'w') as f: \n",
    "        yaml.safe_dump(var_dict, f)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _run_1_exp(variation, fn):\n",
    "    dfs = fn(**variation)\n",
    "    dfs = {k: df.assign(**variation) for k, df in dfs.items()}    \n",
    "    return dfs\n",
    "\n",
    "def run_experiment(fn, var_dict, file_prefix, num_trials=1, \n",
    "                   max_workers=2, chunksize=1):      \n",
    "    variations = create_variations(var_dict, num_trials=num_trials)\n",
    "\n",
    "    results = process_map(\n",
    "        partial(_run_1_exp, fn=fn),\n",
    "        variations, \n",
    "        max_workers=max_workers,\n",
    "        chunksize=chunksize\n",
    "    )\n",
    "\n",
    "    results = save_results(file_prefix, results, variations, var_dict)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d85f818-d29b-4d7a-9c46-78a6718f0063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a9001aed784bfdaf2d4268cf52520e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "var_dict = dict(\n",
    "    num_agents = [20],\n",
    "    social_content = ['belief'],\n",
    "    p_social = [0, 0.5, 1.0],\n",
    "    utility_gamma = [0, 0.5, 1.0],\n",
    "    BMT_error = [3600]\n",
    ")\n",
    "\n",
    "results = run_experiment(\n",
    "    fn = experiment_ER,\n",
    "    var_dict = var_dict,\n",
    "    file_prefix = 'data/output/test/test1', \n",
    "    num_trials = 3,\n",
    "    max_workers = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5682a-b51a-4e38-a13d-d6d7cab68d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca7cf2c30c24d9698e3603f630f2c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run\n",
    "var_dict = dict(\n",
    "    num_agents = [100],\n",
    "    social_content = ['belief',  'reward'],\n",
    "    p_social = [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    utility_gamma = [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    mu0 = [25.0, 100.0], \n",
    "    BMT_error = ['3600', 'use-tasks']\n",
    ")\n",
    "\n",
    "results = run_experiment(\n",
    "    fn = experiment_ER,\n",
    "    var_dict = var_dict,\n",
    "    file_prefix = 'data/output/test/meansocial_childdev', \n",
    "    num_trials = 10,\n",
    "    max_workers = 6,\n",
    "    chunksize = 6\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social-bandit",
   "language": "python",
   "name": "social-bandit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "scenes_data": {
   "active_scene": "Legacy Init",
   "init_scene": "Legacy Init",
   "scenes": [
    "Default Scene",
    "Legacy Init"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
